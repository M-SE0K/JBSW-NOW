#!/usr/bin/env python3
"""
Llama 3.1 8B Fine-tuning ìŠ¤í¬ë¦½íŠ¸ (Unsloth ì‚¬ìš©)

ì‚¬ìš©ë²•:
  1. ì—°êµ¬ì‹¤ PCì— í™˜ê²½ ì„¤ì •:
     pip install unsloth transformers datasets peft accelerate

  2. í•™ìŠµ ë°ì´í„° ì¤€ë¹„:
     data/training_data.jsonl íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.

  3. Fine-tuning ì‹¤í–‰:
     python scripts/finetune-llama.py

  4. í•™ìŠµ ì™„ë£Œ í›„ GGUF ë³€í™˜ ë° Ollama ë“±ë¡

ìš”êµ¬ì‚¬í•­:
  - NVIDIA GPU (12GB+ VRAM ê¶Œì¥)
  - CUDA 11.8+
  - Python 3.10+
"""

import os
import json
import torch
from datasets import Dataset
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments

# ============================================
# ì„¤ì •
# ============================================

# ëª¨ë¸ ì„¤ì •
MODEL_NAME = "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit"  # 4bit ì–‘ìí™” ë²„ì „
MAX_SEQ_LENGTH = 2048
LOAD_IN_4BIT = True

# LoRA ì„¤ì •
LORA_R = 16  # LoRA rank
LORA_ALPHA = 16
LORA_DROPOUT = 0
TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# í•™ìŠµ ì„¤ì •
BATCH_SIZE = 2
GRADIENT_ACCUMULATION_STEPS = 4
LEARNING_RATE = 2e-4
NUM_EPOCHS = 3
WARMUP_STEPS = 10
LOGGING_STEPS = 10
SAVE_STEPS = 100

# ê²½ë¡œ ì„¤ì •
DATA_PATH = "data/training_data.jsonl"
OUTPUT_DIR = "models/llama3.1-8b-jbsw-lora"
FINAL_MODEL_DIR = "models/llama3.1-8b-jbsw-merged"

# ============================================
# ë°ì´í„° ë¡œë“œ
# ============================================

def load_training_data(path):
    """JSONL íŒŒì¼ì—ì„œ í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
    print(f"ğŸ“¥ í•™ìŠµ ë°ì´í„° ë¡œë“œ: {path}")
    
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    
    print(f"  âœ… {len(data)}ê±´ ë¡œë“œ ì™„ë£Œ")
    return data

def format_prompt(instruction, input_text="", output=""):
    """Llama 3.1 Instruct í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
    if input_text:
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ JBSW í†µí•© ì •ë³´ í”Œë«í¼ì˜ ì±—ë´‡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.<|eot_id|><|start_header_id|>user<|end_header_id|>

{instruction}

ì…ë ¥: {input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{output}<|eot_id|>"""
    else:
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ JBSW í†µí•© ì •ë³´ í”Œë«í¼ì˜ ì±—ë´‡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.<|eot_id|><|start_header_id|>user<|end_header_id|>

{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{output}<|eot_id|>"""
    return prompt

def prepare_dataset(data):
    """ë°ì´í„°ë¥¼ Hugging Face Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    formatted = []
    for item in data:
        prompt = format_prompt(
            instruction=item.get("instruction", ""),
            input_text=item.get("input", ""),
            output=item.get("output", "")
        )
        formatted.append({"text": prompt})
    
    return Dataset.from_list(formatted)

# ============================================
# Fine-tuning
# ============================================

def main():
    print("ğŸš€ Llama 3.1 8B Fine-tuning ì‹œì‘\n")
    
    # GPU í™•ì¸
    if not torch.cuda.is_available():
        print("âŒ CUDA GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    print(f"ğŸ–¥ï¸  GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ“Š VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\n")
    
    # ë°ì´í„° ë¡œë“œ
    raw_data = load_training_data(DATA_PATH)
    dataset = prepare_dataset(raw_data)
    print(f"ğŸ“š í•™ìŠµ ë°ì´í„°ì…‹: {len(dataset)}ê±´\n")
    
    # ëª¨ë¸ ë¡œë“œ
    print("ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_NAME,
        max_seq_length=MAX_SEQ_LENGTH,
        load_in_4bit=LOAD_IN_4BIT,
        dtype=None,  # ìë™ ê°ì§€
    )
    print("  âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")
    
    # LoRA ì–´ëŒ‘í„° ì ìš©
    print("ğŸ”§ LoRA ì–´ëŒ‘í„° ì ìš© ì¤‘...")
    model = FastLanguageModel.get_peft_model(
        model,
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        target_modules=TARGET_MODULES,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=42,
    )
    print("  âœ… LoRA ì ìš© ì™„ë£Œ\n")
    
    # í•™ìŠµ ì„¤ì •
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        learning_rate=LEARNING_RATE,
        num_train_epochs=NUM_EPOCHS,
        warmup_steps=WARMUP_STEPS,
        logging_steps=LOGGING_STEPS,
        save_steps=SAVE_STEPS,
        save_total_limit=3,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=42,
        report_to="none",  # wandb ë¹„í™œì„±í™”
    )
    
    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=MAX_SEQ_LENGTH,
        args=training_args,
    )
    
    # í•™ìŠµ ì‹œì‘
    print("ğŸ‹ï¸ Fine-tuning ì‹œì‘...")
    print(f"   - Epochs: {NUM_EPOCHS}")
    print(f"   - Batch size: {BATCH_SIZE}")
    print(f"   - Learning rate: {LEARNING_RATE}")
    print(f"   - LoRA rank: {LORA_R}\n")
    
    trainer.train()
    print("\nâœ… Fine-tuning ì™„ë£Œ!\n")
    
    # LoRA ì–´ëŒ‘í„° ì €ì¥
    print("ğŸ’¾ LoRA ì–´ëŒ‘í„° ì €ì¥ ì¤‘...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"  âœ… ì €ì¥ë¨: {OUTPUT_DIR}\n")
    
    # ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ (ì„ íƒì‚¬í•­)
    print("ğŸ”€ ëª¨ë¸ ë³‘í•© ë° ì €ì¥ ì¤‘...")
    model.save_pretrained_merged(
        FINAL_MODEL_DIR,
        tokenizer,
        save_method="merged_16bit",  # 16bitë¡œ ì €ì¥
    )
    print(f"  âœ… ì €ì¥ë¨: {FINAL_MODEL_DIR}\n")
    
    print("=" * 50)
    print("ğŸ‰ Fine-tuning ì™„ë£Œ!")
    print("=" * 50)
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"1. GGUFë¡œ ë³€í™˜:")
    print(f"   python llama.cpp/convert_hf_to_gguf.py {FINAL_MODEL_DIR} --outtype q4_k_m")
    print(f"\n2. Ollamaì— ë“±ë¡:")
    print(f"   ollama create jbsw-llama -f Modelfile")

if __name__ == "__main__":
    main()

